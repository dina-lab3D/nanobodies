PARAMETERS:

	- activision function (ReLU, leaky ReLU, ELU)
	- learning rate (step size in the Gradient dicsent)
	- number of filters (k)
	- number of strids (s) and size of filters (f) , padding (p)
	- one hot coding / vector representation.
	- how to represent cdr1/cdr2?
	- use recursive network (NLP)? RNN - LSTM
	- how many layers, etc...
	- preprocces data ? (zero centered? normalized?)
	- wights insalization (Xavier, of Xavier/2)
	- Batch normalization
	- SGD + Momentum / Nesterov / RMSProp / Adam ( 0.9/0.99)
	- learning rate decay?
	- dropout (for regulazation)
	- in the end - use t-sne to cluster the final vectors 


	
